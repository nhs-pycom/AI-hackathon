
# Community Survey Responses

These are the responses collected from the NHS Python Community Large Language Models (LLMs) Hackathon Survey.

We have included the responses to the following questions:


> Q1. Please provide any suggestions you have for the hackathon e.g. an idea, a challenge, or a pain point you think LLMs could aid in addressing. (Suggestions)

> Q2. Please give further details (where possible) on the current situation surrounding your previous answer, why this is not ideal, and what your vision of the best-case scenario would be. (Current Situation / Future Vision)

> Q3. Please give any details on existing components you are aware of that could support your idea, or thoughts on any resources that would need preparing ahead of the event e.g. (open) data sources or synthetic datasets, useful frameworks, models, etc. (Resources / Preparation)

| Q1. Suggestions | Q2. Current Situation / Future Vision | Q3. Resources / Preparation |
| :---        |    :----   |          :--- |
| I saw a recent figure with ChatGPT was to upload images that the model could read/parse | A big part of my work is preparing a large monthly slide deck. I've been working on automating it as much as possible because at first creating the next month's pack took up the majority of my month. The one area I have not been able to automate is the narrative that accompanies the visuals, and this is currently a manual process. I would be interested if there was a way I could semi-automate this with generated narrative that I just need to review/touch-up. | If this was for a challenge then a set of visuals to generate narrative for. |
| Development of a chatbot to help technical leads and developers interrogate clinical techniques and best practices to aid in decision making during the development process. | -    |  -   |
| 1. Methods to prevent the sharing of secure data <br />2. Methods to recognise hallucination. | 1. We donâ€™t want folks sharing secure health/personal data with third-party LLMs.  <br />2. We need to avoid factually-incorrect health-related information being provided.     | Hm, tricky, <br />1. Some kind of input front-end to catch potential security breaches in user inputs?  <br />2. A system to compare outputs against published info, or maybe a system for users to flag noticeably-incorrect outputs?     |
| Identify the public domain health sector data that could be used to most efficiently pre-train an LLM for a trustworthy health AI application | This answer is not ideal because of the challenge associated with confirming that the LLM application being developed is trustworthy, ethical and responsible. | A variety of resources are available to understand the requirements for trustworthy, ethical and responsible AI (e.g. from EU or UNESCO expert communities.)  <br />This doesn't make it any easier. |
|  A bespoke chatbot for NHS analysts and stakeholders new to Data Science and want to know more specific to their needs   |  There are many overlapping websites and for those not familiar with slack, there might be a barrier to entry for learning good practice and coding techniques. People might be more comfortable asking a chatbot something as opposed to feeling able to ask a person. <br />Example questions:<br /><ul><li>What is RAP and why should I care?</li><li>Are there projects in X field where good practice has been used?</li><li>Where is AI used in the NHS? Should I use it in my job?</li><li>Is there someone in field X or location Y or organisation Z who is skilled in certain things?</li></ul>   |   I have seen people use ChatGPT embedded within their website as a website-specific chatbot  |
|   Finding relevant data APIs and constructing example requests for them   |   Currently might goto the NHS Digital API catalogue and search for an app and then read the docs to construct a request.   |  Have heard a lot about Langchain but have never used it.    |
|  Patient questions around managing a given diagnosis    |  -   |   -  |
|   Is there a way we can use python to hear data? The human ear is much better at sensing differences than our sites is? Can we turn data numbers into a pitch that can then change depending on number levels.  |  And we could have a pre-existing visualization or a new one that has a button next to it so you could hear what the data sounds like in different tones. <br />It also could be seen as a inclusive fatigue feature.    |   I know some python modules deal with converting input into sound and notes. I think it's a good thing to learn different libraries and we'll involve the stringing a few things together.   |
|  <ul><li>Answering questions regarding patient's medical history including medication.</li><li>Answering questions regarding local policy and clinical guidelines. </li><li>Suggesting optimal treatment based on local guideline documents/policies and medical history.</li></ul>  |   Guidelines/policies may be difficult to find for new starters at organizations. It may also take too many clicks to find the right care plan. <br />An LLM may stream line the process by providing answers, links, steps etc.  |  NICE guidelines are available on the web. Could scrape NICE guidelines into a PDF and train an LLM on the text.   |
|  1) We've talked about whether we can sit and LLM on our data warehouse and rather than have dashboards to be able to ask questions of the tables directly. I'd be interested in the insights and how those compare to more traditional visualisation tools.<br />2) The other most common idea is to web scrape all ICS organisations websites and have one central LLM to query all ICS public sector documents/web pages and create a one stop shop. There are several LLM's that already do this though e.g. ChatBase    |   -  |   -  |
|  An aid to often lengthy performance reporting through a command line. e.g. Here are generic performance graphs, for further information please use a cmd prompt to "Please tell me the top five services in terms of DNAs".    |   Exec et al. currently receive raft of figures and reporting around all elements of services, reporting designers must predict what interpreters will require which is sub-optimal. Instead, this could be a means to facilitate interpreters to find answers they require.  |   Langchain seems like it could be a way forward: <https://python.langchain.com/docs/get_started/introduction>.  |
|   An open source tool for analysing written consultation responses in organsations who do not have large data science teams   |   Public consultations on policy changes can generate a lot of written responses.  <br />Less tech mature organisations do not have the facility to use ML and LLMs to analyse these responses so use manual processes to label the data. This is slow, costly, inefficient and prone to bias.  <br />Also data are sometimes sensitive so public API's and cloud solutions are risky.  <br />A secure analytical pipeline which automatically augments your consultation responses with meaningful topics and then provides analysis of those topics on selected strata from categorical answers in the consultation would be valuable and cost saving for a lot of orgs in the NHS and beyond.  <br />The solution will also need good consideration of bias and ethnical implementation - thinking here about accessibility, literacy levels, and non-native speakers and also the maintaining the views of people who do not provide written inputs.   |  Using BERTopic will give you topic segments - you could use a LLM to give them a better description (rather than a list of key words) - these require either a GPU or use of APIs. <br />Alternatively you could go down a NLTK route which might play nicer locally. <br />Data ingestion and analysis can be handled by Python. <br />You could look at vector databases too.     |
| Use Case 1: Evidence synthesis for rare diseases, with applications for phenotyping and treatment planning. <br />Rationale: Evidence synthesis powered by Large Language Models (LLMs) could have played a crucial role in helping to respond more effectively to epidemics and pandemics more effectively, particularly in the early stages, where the spreading mechanisms and treatment options were speculative. An LLM, using the clinical presentation of a patient, could have summarised the literature of similar disease presentations along with the relevant treatment guidelines to facilitate more rapid and informed decision making. <br />Data Sources: Publicly available medical literature, clinical trial data and case reports. <br />Scope for completion: Depending on the compute it would be possible to complete the process of data scraping, LLM tuning and demonstration on an example disease (COVID) within 3 days.   |  Use case 1: The current situation relies heavily on researchers and clinicians completing the literature search and skim reading documents in their own time, as there is little scope for such work during routine practice. Best case scenario where clinical presentations could be entered and a summary of the existing literature surrounding the presentation is provided. Additionally, recommendations could be provided with rationale provided for recommendations.    |   Use Case 1: Retrieval-Augmented Generation methods could be used to substantiate the responses of an AI with scientific evidence.   |
|  Use Case 2: Healthcare Information Summariser for Patients, with capabilities to alter the reading age and language summarisation. If there was scope, it would be cool to see if the proposed approaches could be applied in real-time as a doctor is speaking, acting as a translation service (although this data may be publicly available). <br />Rationale: Healthcare Information is of importance to all patients however, some patients, particularly those in deprived areas may have poor reading comprehension or be non-English speakers. <br />Data Sources: Publicly available medical literature, clinical guidelines, patient notification documents. <br />Scope for completion: Depending on the compute it would be possible to entire project, minus the real time element within a 3 day period    |   Use case 2: Currently a heavy reliance on translators, family and friends to perform the translation and explanation of such documents. Best-case outcome would be a front end system where patient information documents could be uploaded, reading age and language selected, and the document tailored to the patient.   |   Use Case 2: May be a bit simple, but some API to existing translation services with an LLM to clean up the translation may be useful.    |
|  Interfacing a LLMs to multiple EPMAs/EPR solutions via FHIR or HL7. <br />Providing a summary of a patient based on a prompt and pulling relevant data/documentation/clinical result of the patient.    |  Currently all this is manual, best case scenario is a few word prompt to give a summary of a patient, which the clinician can then drill down more if required. Different professions will want different information from this summary though.   |  <ul><li><https://fhir.epic.com/Developer/></li><li><https://code.cerner.com/start-coding></li><li><https://home.meditech.com/en/d/restapiresources/homepage.html></li></ul>    |
|  Translating clinic letters for people who English is not their first language.   |  I work in an area where there is a huge amount of stigma attached to the conditions I manage. Many people do not speak English. To support people to take ownership over their healthcare in the same way an English-speaking person does. Foreign language letters would support this. To my knowledge this doesn't exist.    |   ChatGPT has translation capabilities. However, the main limitation would be checking to ensure no mistakes/hallucinations programmatically.  |
|  Waiting List is always an issue , for all modalities the  there are 6 Week to 104 Week waiters for Diagnostic tests, If we can arrange some case study to overcome the waits will be helpful.    |   Deliver diagnostic activity levels that support plans to address elective and cancer backlogs and the diagnostic waiting time ambition, Aim is to increase the percentage of patients that receive a diagnostic test within six weeks in line with the March 2025 ambition of 95%.  |   The published WLMDS is open data source that we can study to establish the future predictive model   |
|  There is lots of information relating to patient discharge activity that is stored in unstructured text. LLMs could parse clinical notes for signals of (a) whether a patient is likely to be discharged today and (b) what they are waiting for  (contacting family members, packages of care, ordering take-home medication, booking transport, clinical review, lab results, discharge summaries) that is holding them back from vacating a bed.  <br />The hackathon could either train a model to do this parsing or (more useful) generate notes that could be used to train such a model in future.    |   I've worked extensively with bed planners in an acute hospital. They are always keen to know exactly which beds will be free, and when, to assist in planning for new patients waiting in A&E to be admitted. This is a huge problem right now, as most UK hospitals are at capacity meaning that patients awaiting beds have to reside in A&E until their beds are ready.  <br />In particular, bed planners would like to know further in advance whether a patient is suitable to be sent to a discharge lounge, and whether they qualify for hospital transport paid for by the NHS, which ideally should be booked in advance. <br />This, and other information about what patients are waiting for, is often recorded unstructured text. A best-case scenario would be a NLP-based model scanning notes in real-time and summarising outstanding tasks that could be viewed at a glance. <br />However, this is difficult to achieve because suitable training data does not exist. Synthetic patient notes that are typical of inpatient hospital visits would be ideal. (MIMIC notes are not suitable as patients are rarely discharged directly from ICU.) Ideally, there would be notes that a LLM could train on that would reflect the diversity of notes that are recorded during a hospital visit (including medical and nursing notes, but also notes from occupational and physiotherapists, complex discharge teams and social workers.)   |   I have been experimenting with using Synthea as a starting point for the generation of such notes.   |
|   There is an effort underway to create a website with information on regulation, guidance and standards for NHS staff looking to develop or commission AI products. Furthermore, the website will have information on AI use cases in health and social care, events and other resources.  There is a lot of text on the website - pages and pages of guidance documents to read through to find relevant information, as well as links to other websites with more information. With the time pressures that clinicians and other healthcare workers have, reducing any time barrier in finding information would be a welcomed endeavour.   |   I propose enhancing web search functionality by integrating AI chatbot to provide a more natural and interactive search experience, or building a system that can take information publicly available from multiple websites to allow users to search relevant information, without needing to visit each website.   |   This is the website currently being developed: <https://www.digitalregulations.innovation.nhs.uk/> <br />I also came across ONS efforts to improve user web search experience with LLMs: <https://datasciencecampus.ons.gov.uk/using-large-language-models-llms-to-improve-website-search-experience-with-statschat/>   |
